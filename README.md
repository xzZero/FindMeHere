<h1 align="center">Hi there, I'm Son ðŸ‘‹</h1>

Here, I am writing about my learning journey with coding. I am a fresh graduate of Stuttgart University majoring in Computer Hardware/Software Engineering. Besides, I have a strong background in Embedded Systems through my bachelor, working and research experience. I have a passion for coding, especially logic and number. In my third year as a Bachelor, I had a chance to develop wireless air quality sensing systems. It contained hundreds of nodes and sent a tremendous amount of data in the blink of an eye. My team and I used the data from sensors to draw an air quality map and to forecast if the users were in a danger zone (bad air quality). From that day on, I knew the potential of such big data and also the burden of processing and gathering them. With the target to be sufficient for the data world, I have been working non-stop to broaden my knowledge and foster my programming skills.

My interests: Automating stuffs, Data Engineering and Competitive Coding

Below are some projects that I have been working on:
## [IBM Data Engineering](https://github.com/xzZero/DataEng_IBM/tree/main/13%20-%20Data%20Engineering%20Capstone%20Project)
This is my latest project where I attended an intensive professional course by IBM. I was amazed at myself when I was able to handle a 54-week course in less than 3 weeks. Mainly because I was broke (still have no money now) and I chose "pay as you go" for one month only on coursera. 

In this project, I built a data platform architecture for an e-commerce company. Since the company uses a hybrid architecture, some of its databases are on-premises while others are on the cloud. The company gathers data mostly through its website, and the data is first stored and handled in OLTP Database (built in MySQL) for transactional data and NoSQL Database (built in MongoDB) for catalog data. So it is for the webserver. The data is periodically extracted from these 2 databases and put into the staging data warehouse running on PostgreSQL. Meanwhile, the production data warehouse is on the cloud using IBM Db2. The dashboard is created by IBM Cognos Analytics by connecting to the production data warehouse. Spark is also acquired to analyze the data on the Hadoop cluster, and Linear Regression is used to predict the sales of the next year. Additionally, to move the data between OLTP, NoSQL and the data warehouse, ETL pipelines are implemented and run on Apache Airflow.

Besides the capstone project, I also documented my entire course in this [link](https://github.com/xzZero/DataEng_IBM).
## [Automation of Test cases Creation](https://www.robo-test.ai/)
In this project, I contributed to an incubator named Robo-Test hosted by the Institute of Industrial Automation and Software Engineering at Stuttgart University. Robo-Test provides a platform and toolbox for test and release of automated and autonomous systems. In recent years, manual testing has been showing its ineffectiveness in autonomous driving testing due to the need for the on-the-fly corrections and changes in autonomous systems.

I successfully developed a Web-based Application using Django to automate the scenario-based test cases creation from DOORS requirements. There are three main challenges to the project: (1) How to convert the textual natural language requirements to the verification criteria in Signal Temporal Logic (which is a formal language)?; (2) How to generate python scripts for test cases from requirements (test cases are written in Python)?; (3) How to reuse the test cases in other testing systems (the need of a test case standard)?

To solve the first challenge, an NLP model - [DeepSTL](https://github.com/JieHE-2020/DeepSTL) was used to convert the natural language text to verification criteria in Signal Temporal Language (STL). By implementing the model, the accuracy was **80%** for the unrestricted test condition, and **95%** for the test condition with restricted boundary followed [EARS Pattern](https://alistairmavin.com/ears/). As for the second challenge, I came up with a test case template which can be fit to any test cases in the systems. The final challenge was the most difficult, since no test case standard is capable of transferring and reusing the test cases/test results in ADAS (up to my knowledge, even with ASAM - one of the leaders in ADAS/AD Testing). Then, I found TestIF in a late night. TestIF was not meant to be designed to work with ADAS/AD testing, however, it supports customization based on user's needs unlike other standards. After that, I did some magics with TestIF, and it fully supports the scenario-based test cases generation.

Beside these tasks, I connected all the components of the testing systems, from DOORS, and simulation tools (LGSVL, VTD) to my Web App. The developed application ended up with a speed-up factor of **50x** in creating test cases automatically as compared to the manual approach. And I was given 1.3/1.0 in German Scale, approximately 9.5/10.0 for this Master thesis.

## [Air Quality Monitoring Systems in Smart City](https://vgu.edu.vn/vi/achievements1?title=VGU-EEIT-students-got-the-first-prize-in-Hackathon-contest-2017&id-bai-viet=2735808&pid=CmsHienThiBaiViet_WAR_cmsportlet_INSTANCE_1xni0P8F64Cp&reCall=1)
This project is what leads me to this career path. Its idea first sparked in my head when my team and I just really wanted to land our hands on the lowest prize in a national hackathon. At that time (2017), people in Vietnam were scared of the environment they were living. They all stuck and believed in a commercial app called AirVisual. But can we believe the app? I asked myself. So I had to have my hands dirty.

In this project, my team and I constructed a wireless low-cost air quality monitoring system. Our strategy was to spread as many nodes around the city as possible to increase the density, so the data would be always kept alive and reliable. The nodes were communicated (via HTTP) to the data warehouse hosted in Google Sheets, and controlled via Google Apps Script. We were poor, and we, as university students, were provided with an unlimited drive, so we took this advantage. The Apps Script was responsible for gathering the data sent from the nodes, putting the data into Sheets, aggregating data and sending it to Firebase. I used Firebase as a Web driver and the backend for the Android app to show the users an air quality map or to warn the users of bad air condition. My tasks were to develop the web server, APIs, and Android App. Thanks to my team for doing a great part with the hardware (node), it reacted so quickly to a bad environment. Therefore, it took less than 2 minutes when the node sensed bad air condition until the App alerted the user.

Surprisingly, we won the first prize along with two other prizes (Start-up Support for Outstanding Team, IBM Grant for Best Performance Team). 

After winning, we had upgrade the architecture to use both HTTP and MQTT for the communication of nodes. We also spread more nodes, had more data, use the proper server and database, and added Machine Learning and BI with IBM Watson and IBM Cognos (Since we had IBM Support, lol, never thought we could afford it). But, it is enough for today, I promise I will write more about the current version of the systems on another day.
## [GenSeT Reconstruction with GPU](https://github.com/xzZero/GenSeT-CUDA)
During my bachelor years, I conducted a research in Magnetic Resonance Imaging (MRI), especially compressed sensing reconstruction. In this project, I implemented a parallel version of the GenSeT reconstruction model. GenSeT is an advanced method of GS (Generalized Series) with the support of the Nesterov method, and the guideline of full-precision first image. The outcome of the parallel implementation in CUDA was a **48x** faster than the traditional serial version in CPU. My proposed method was published at the 18th IEEE International Symposium on Signal Processing and Information Technology (ISSPIT) in the USA.
## [GS Reconstruction with GPU](https://github.com/xzZero/GS-CUDA)
I started working on this project before the GenSet. I used the know-how of this project as a stepping stone to my way to CUDA. GS or Generalized Series is a method to reconstruct the sparsely sampled image in MRI, while the traditional methods cannot due to Nyquist violation. Undeniably, it takes a tremendous amount of time to reconstruct these violated images. To speed up the computational process, I acquired several advanced CUDA approaches like unrolling loops, read-only cache, cuBLAS, ... The achieved speed-up factor was **40x** with fMRI data. With this project, I was given 1.0/1.0 German scale, approximately 10.0/10.0 for the Bachelor thesis.
## [I2C Reversing](https://github.com/xzZero/I2C_traceback)
This project was a part of my learning curve to deal with I2C is the rear lighting systems in cars (mostly with expensive cars - Mercedes S class). I began this when my boss gave me a task to improve the signal of I2C protocol between the HiL and the lighting module. The facing problems are data loss, unreliable transmission signal, and bit flip due to long-distance of the line between HiL and the testing module (I2C is designed to work well in short-distance like break board only). To learn more about I2C, I wrote this simple code to do the reversed engineering on the Atmega 2560. The I2C signal was driven from the Atmega 2560 to the 16-channel 12-bit PWM servo PCA9685 interface to control the lighting of the LED, and the signal was read using an oscilloscope. 

Later, I used the findings to develop an I2C-CAN translator to translate the I2C data to CAN (backward and forward) from the car lighting module and HiL. The translator achieved **99%** uptime and consistency (Tested 10000 times with automation codes).
## [Awesome Coding Stuffs](https://github.com/xzZero/Competitive-Coding-)
I created this repo just to enjoy my fun after solving coding challenges. This repo documented some of my solutions, tips and tricks to the competitive coding challenges I attended. Currently, I am uploading the solutions for Project Euler+ in Hackerrank. I used to rank at 20 in Germany, fell to 22 now :((. Because manually uploading code from hackerrank is tedious and time-consuming, I have been working on a project to automate this process. That's the reason why I have not uploaded any ever since, also due to the pressure of my Master thesis. 
## [Dog Breed Classifier](https://github.com/xzZero/Dog_Breed_Classifier)
This project deploys 2-step classifications: 
- Classifying Human vs. Dog
- Classifying Dog Breed: If a dog is found, then find the dog breed

The dog breed model was an attempt to learn transfer learning method. I deployed 3 pre-trained models (VGG-16, Inception V3, Resnet-50), Resnet-50 was the best after all. There are 133 breeds involved in the training and the accuracy was **83%**. The model was deployed in S3 and trained in Sage Maker.
## [Plagiarism Detection](https://github.com/xzZero/Plagiarism_Detection)
This project deploys a plagiarism detector that takes a text file and performs binary classification based on Linear SVC provided by SKLearn. To decide if the text is plagiarized, containment feature and longest common subsequence are implemented. Accuracy achieved: **96%**
## [Sentimental Prediction](https://github.com/xzZero/sentimental_prediction)
In this project, I deployed an LSTM model with Word Embeddings on a Web Service using AWS Lambda and AWS's API. The website took a comment out of IMDB and predicted if the commenter's feeling was negative/positive. The model achieved **98%** accuracy.
## Android Apps
Some of my Android Apps in my free time:
- [Weather App](https://github.com/xzZero/WeatherAppv2): shows the weather condition in different cities in Germany
- [Calculator](https://github.com/xzZero/Calculator): performs basic calculations (addition, subtraction, multiplication, and division) including brackets in math equations
- [Position Tracker](https://github.com/xzZero/Positioning): A clone of Strava to track the average speed, distance and location.
